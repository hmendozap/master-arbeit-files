{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import load_detailed_prepro as ldp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'load_detailed_prepro' from '/home/hmendoza/workspace/master_arbeit/auto-deep/parameters_curves/load_detailed_prepro.py'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(ldp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data location and dataset\n",
    "data_dir = '/home/hmendoza/workspace/master_arbeit/auto-deep/nn_prepro_Experiment/results/experiment/'\n",
    "dataset = '1128_bac'\n",
    "preprocessor='select_rates'\n",
    "fline = 'detailed-traj-run-10.csv'\n",
    "traj_file = os.path.join(data_dir, dataset, preprocessor, dataset, fline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# But we also get a DataFrame with the data (can also be done in R)\n",
    "tdf = ldp.load_trajectories(data_dir, dataset, preprocessor='no_preprocessing',\n",
    "                            full_config=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61, 30)\n"
     ]
    }
   ],
   "source": [
    "# Start filtering the error (Don't know if add CRASHED configurations)\n",
    "err_threshold = 1.0\n",
    "less_threshold = True\n",
    "\n",
    "if less_threshold:\n",
    "    filtered = tdf[tdf.performance <= err_threshold]\n",
    "else:\n",
    "    filtered = tdf[tdf.performance >= err_threshold]\n",
    "        \n",
    "print(filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.000000\n",
       "1    0.994565\n",
       "2    0.985535\n",
       "3    0.905582\n",
       "4    0.893520\n",
       "Name: performance, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered.performance[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coded_df = filtered.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            batch_size\n",
       "1                 beta1\n",
       "2                 beta2\n",
       "3       dropout_layer_1\n",
       "4       dropout_layer_2\n",
       "5       dropout_layer_3\n",
       "6       dropout_layer_4\n",
       "7       dropout_layer_5\n",
       "8       dropout_layer_6\n",
       "9        dropout_output\n",
       "10        learning_rate\n",
       "11             momentum\n",
       "12           num_layers\n",
       "13    num_units_layer_1\n",
       "14    num_units_layer_2\n",
       "15    num_units_layer_3\n",
       "16    num_units_layer_4\n",
       "17    num_units_layer_5\n",
       "18    num_units_layer_6\n",
       "19        number_epochs\n",
       "20                  rho\n",
       "21               solver\n",
       "22          std_layer_1\n",
       "23          std_layer_2\n",
       "24          std_layer_3\n",
       "25          std_layer_4\n",
       "26          std_layer_5\n",
       "27          std_layer_6\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(coded_df.ix[:,1:-1].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding used in solver and preprocessor categorical variables\n",
    "adadelta  0, adagrad   1, adam      2, momentum  3, nesterov  4, sgd       5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try and work how the rfreg behaves with indicator variables\n",
    "# http://pandas.pydata.org/pandas-docs/version/0.17.0/reshaping.html#computing-indicator-dummy-variables\n",
    "coded_df['solver'] = filtered['solver'].astype('category')\n",
    "coded_df['preprocessor'] = filtered['preprocessor'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>performance</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>beta1</th>\n",
       "      <th>beta2</th>\n",
       "      <th>dropout_layer_1</th>\n",
       "      <th>dropout_layer_2</th>\n",
       "      <th>dropout_layer_3</th>\n",
       "      <th>dropout_layer_4</th>\n",
       "      <th>dropout_layer_5</th>\n",
       "      <th>dropout_layer_6</th>\n",
       "      <th>...</th>\n",
       "      <th>number_epochs</th>\n",
       "      <th>rho</th>\n",
       "      <th>solver</th>\n",
       "      <th>std_layer_1</th>\n",
       "      <th>std_layer_2</th>\n",
       "      <th>std_layer_3</th>\n",
       "      <th>std_layer_4</th>\n",
       "      <th>std_layer_5</th>\n",
       "      <th>std_layer_6</th>\n",
       "      <th>preprocessor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.078308</td>\n",
       "      <td>854</td>\n",
       "      <td>0.196384</td>\n",
       "      <td>0.913762</td>\n",
       "      <td>0.084144</td>\n",
       "      <td>0.861973</td>\n",
       "      <td>0.171859</td>\n",
       "      <td>0.904045</td>\n",
       "      <td>0.019498</td>\n",
       "      <td>0.921645</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.017938</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.032325</td>\n",
       "      <td>0.021721</td>\n",
       "      <td>0.067461</td>\n",
       "      <td>0.086482</td>\n",
       "      <td>0.087822</td>\n",
       "      <td>0.071028</td>\n",
       "      <td>no_preprocessing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.082602</td>\n",
       "      <td>275</td>\n",
       "      <td>0.676804</td>\n",
       "      <td>0.574869</td>\n",
       "      <td>0.577597</td>\n",
       "      <td>0.589949</td>\n",
       "      <td>0.888228</td>\n",
       "      <td>0.712073</td>\n",
       "      <td>0.262108</td>\n",
       "      <td>0.909249</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.413255</td>\n",
       "      <td>momentum</td>\n",
       "      <td>0.058553</td>\n",
       "      <td>0.040004</td>\n",
       "      <td>0.053026</td>\n",
       "      <td>0.053220</td>\n",
       "      <td>0.088900</td>\n",
       "      <td>0.006481</td>\n",
       "      <td>no_preprocessing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.082664</td>\n",
       "      <td>185</td>\n",
       "      <td>0.274779</td>\n",
       "      <td>0.058622</td>\n",
       "      <td>0.858087</td>\n",
       "      <td>0.667039</td>\n",
       "      <td>0.877792</td>\n",
       "      <td>0.357267</td>\n",
       "      <td>0.682999</td>\n",
       "      <td>0.958286</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.100290</td>\n",
       "      <td>momentum</td>\n",
       "      <td>0.044592</td>\n",
       "      <td>0.062359</td>\n",
       "      <td>0.010090</td>\n",
       "      <td>0.099593</td>\n",
       "      <td>0.007928</td>\n",
       "      <td>0.025462</td>\n",
       "      <td>no_preprocessing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.083246</td>\n",
       "      <td>724</td>\n",
       "      <td>0.314150</td>\n",
       "      <td>0.634334</td>\n",
       "      <td>0.335285</td>\n",
       "      <td>0.566046</td>\n",
       "      <td>0.204832</td>\n",
       "      <td>0.290946</td>\n",
       "      <td>0.848140</td>\n",
       "      <td>0.514260</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.118770</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.016347</td>\n",
       "      <td>0.031210</td>\n",
       "      <td>0.058050</td>\n",
       "      <td>0.089883</td>\n",
       "      <td>0.014399</td>\n",
       "      <td>0.011898</td>\n",
       "      <td>no_preprocessing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.085101</td>\n",
       "      <td>175</td>\n",
       "      <td>0.907804</td>\n",
       "      <td>0.758103</td>\n",
       "      <td>0.808820</td>\n",
       "      <td>0.965795</td>\n",
       "      <td>0.019026</td>\n",
       "      <td>0.403560</td>\n",
       "      <td>0.033201</td>\n",
       "      <td>0.548311</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.869560</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.087493</td>\n",
       "      <td>0.095189</td>\n",
       "      <td>0.097195</td>\n",
       "      <td>0.065028</td>\n",
       "      <td>0.072724</td>\n",
       "      <td>0.045124</td>\n",
       "      <td>no_preprocessing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.087579</td>\n",
       "      <td>445</td>\n",
       "      <td>0.741600</td>\n",
       "      <td>0.522028</td>\n",
       "      <td>0.525227</td>\n",
       "      <td>0.609476</td>\n",
       "      <td>0.956937</td>\n",
       "      <td>0.893637</td>\n",
       "      <td>0.324273</td>\n",
       "      <td>0.080875</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.820418</td>\n",
       "      <td>momentum</td>\n",
       "      <td>0.007341</td>\n",
       "      <td>0.022111</td>\n",
       "      <td>0.029298</td>\n",
       "      <td>0.057126</td>\n",
       "      <td>0.026064</td>\n",
       "      <td>0.072571</td>\n",
       "      <td>no_preprocessing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.090063</td>\n",
       "      <td>228</td>\n",
       "      <td>0.432863</td>\n",
       "      <td>0.431075</td>\n",
       "      <td>0.457242</td>\n",
       "      <td>0.064014</td>\n",
       "      <td>0.126367</td>\n",
       "      <td>0.287198</td>\n",
       "      <td>0.543494</td>\n",
       "      <td>0.825455</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.375674</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.060206</td>\n",
       "      <td>0.082668</td>\n",
       "      <td>0.010651</td>\n",
       "      <td>0.097712</td>\n",
       "      <td>0.086164</td>\n",
       "      <td>0.054308</td>\n",
       "      <td>no_preprocessing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.091320</td>\n",
       "      <td>271</td>\n",
       "      <td>0.786533</td>\n",
       "      <td>0.277555</td>\n",
       "      <td>0.208178</td>\n",
       "      <td>0.828025</td>\n",
       "      <td>0.512164</td>\n",
       "      <td>0.809649</td>\n",
       "      <td>0.492633</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.526185</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.041382</td>\n",
       "      <td>0.029106</td>\n",
       "      <td>0.058355</td>\n",
       "      <td>0.076481</td>\n",
       "      <td>0.021931</td>\n",
       "      <td>0.062448</td>\n",
       "      <td>no_preprocessing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.092548</td>\n",
       "      <td>270</td>\n",
       "      <td>0.953142</td>\n",
       "      <td>0.160750</td>\n",
       "      <td>0.588261</td>\n",
       "      <td>0.135091</td>\n",
       "      <td>0.321836</td>\n",
       "      <td>0.281384</td>\n",
       "      <td>0.294965</td>\n",
       "      <td>0.164420</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.552695</td>\n",
       "      <td>nesterov</td>\n",
       "      <td>0.023848</td>\n",
       "      <td>0.071060</td>\n",
       "      <td>0.096307</td>\n",
       "      <td>0.040230</td>\n",
       "      <td>0.086203</td>\n",
       "      <td>0.097796</td>\n",
       "      <td>no_preprocessing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.093169</td>\n",
       "      <td>171</td>\n",
       "      <td>0.152366</td>\n",
       "      <td>0.650134</td>\n",
       "      <td>0.808500</td>\n",
       "      <td>0.881198</td>\n",
       "      <td>0.285800</td>\n",
       "      <td>0.593314</td>\n",
       "      <td>0.074184</td>\n",
       "      <td>0.274967</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0.464284</td>\n",
       "      <td>nesterov</td>\n",
       "      <td>0.030270</td>\n",
       "      <td>0.091690</td>\n",
       "      <td>0.025418</td>\n",
       "      <td>0.070358</td>\n",
       "      <td>0.037071</td>\n",
       "      <td>0.007976</td>\n",
       "      <td>no_preprocessing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.093184</td>\n",
       "      <td>371</td>\n",
       "      <td>0.961718</td>\n",
       "      <td>0.073293</td>\n",
       "      <td>0.510738</td>\n",
       "      <td>0.473444</td>\n",
       "      <td>0.545795</td>\n",
       "      <td>0.510467</td>\n",
       "      <td>0.041528</td>\n",
       "      <td>0.223776</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0.269312</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.053687</td>\n",
       "      <td>0.060496</td>\n",
       "      <td>0.074062</td>\n",
       "      <td>0.066738</td>\n",
       "      <td>0.068031</td>\n",
       "      <td>0.055623</td>\n",
       "      <td>no_preprocessing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.098759</td>\n",
       "      <td>397</td>\n",
       "      <td>0.368593</td>\n",
       "      <td>0.805288</td>\n",
       "      <td>0.599187</td>\n",
       "      <td>0.775147</td>\n",
       "      <td>0.631153</td>\n",
       "      <td>0.247123</td>\n",
       "      <td>0.861399</td>\n",
       "      <td>0.531999</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.027757</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.078539</td>\n",
       "      <td>0.026020</td>\n",
       "      <td>0.053881</td>\n",
       "      <td>0.066439</td>\n",
       "      <td>0.006898</td>\n",
       "      <td>0.079550</td>\n",
       "      <td>no_preprocessing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.099303</td>\n",
       "      <td>206</td>\n",
       "      <td>0.036937</td>\n",
       "      <td>0.406246</td>\n",
       "      <td>0.843102</td>\n",
       "      <td>0.010373</td>\n",
       "      <td>0.035298</td>\n",
       "      <td>0.657890</td>\n",
       "      <td>0.057009</td>\n",
       "      <td>0.223576</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>0.354519</td>\n",
       "      <td>nesterov</td>\n",
       "      <td>0.095345</td>\n",
       "      <td>0.080537</td>\n",
       "      <td>0.072212</td>\n",
       "      <td>0.075598</td>\n",
       "      <td>0.002793</td>\n",
       "      <td>0.090751</td>\n",
       "      <td>no_preprocessing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.100592</td>\n",
       "      <td>171</td>\n",
       "      <td>0.152366</td>\n",
       "      <td>0.650134</td>\n",
       "      <td>0.808500</td>\n",
       "      <td>0.881198</td>\n",
       "      <td>0.285800</td>\n",
       "      <td>0.593314</td>\n",
       "      <td>0.074184</td>\n",
       "      <td>0.274967</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0.464284</td>\n",
       "      <td>nesterov</td>\n",
       "      <td>0.030270</td>\n",
       "      <td>0.091690</td>\n",
       "      <td>0.025418</td>\n",
       "      <td>0.070358</td>\n",
       "      <td>0.037071</td>\n",
       "      <td>0.007976</td>\n",
       "      <td>no_preprocessing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.101266</td>\n",
       "      <td>826</td>\n",
       "      <td>0.559234</td>\n",
       "      <td>0.795851</td>\n",
       "      <td>0.215344</td>\n",
       "      <td>0.818202</td>\n",
       "      <td>0.793165</td>\n",
       "      <td>0.885243</td>\n",
       "      <td>0.396036</td>\n",
       "      <td>0.275218</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.248684</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.059479</td>\n",
       "      <td>0.040990</td>\n",
       "      <td>0.056198</td>\n",
       "      <td>0.082273</td>\n",
       "      <td>0.027011</td>\n",
       "      <td>0.037775</td>\n",
       "      <td>no_preprocessing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.102501</td>\n",
       "      <td>394</td>\n",
       "      <td>0.764309</td>\n",
       "      <td>0.500972</td>\n",
       "      <td>0.468970</td>\n",
       "      <td>0.708864</td>\n",
       "      <td>0.571474</td>\n",
       "      <td>0.600712</td>\n",
       "      <td>0.752166</td>\n",
       "      <td>0.401412</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>0.361452</td>\n",
       "      <td>adadelta</td>\n",
       "      <td>0.055455</td>\n",
       "      <td>0.093656</td>\n",
       "      <td>0.031279</td>\n",
       "      <td>0.053957</td>\n",
       "      <td>0.079899</td>\n",
       "      <td>0.079330</td>\n",
       "      <td>no_preprocessing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.104427</td>\n",
       "      <td>152</td>\n",
       "      <td>0.426774</td>\n",
       "      <td>0.378753</td>\n",
       "      <td>0.357967</td>\n",
       "      <td>0.632846</td>\n",
       "      <td>0.112174</td>\n",
       "      <td>0.142758</td>\n",
       "      <td>0.230819</td>\n",
       "      <td>0.156980</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.047387</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.057602</td>\n",
       "      <td>0.026257</td>\n",
       "      <td>0.061049</td>\n",
       "      <td>0.035945</td>\n",
       "      <td>0.080969</td>\n",
       "      <td>0.050337</td>\n",
       "      <td>no_preprocessing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.107462</td>\n",
       "      <td>164</td>\n",
       "      <td>0.558045</td>\n",
       "      <td>0.405515</td>\n",
       "      <td>0.611085</td>\n",
       "      <td>0.723711</td>\n",
       "      <td>0.916237</td>\n",
       "      <td>0.545445</td>\n",
       "      <td>0.268705</td>\n",
       "      <td>0.530799</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>0.461751</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.087097</td>\n",
       "      <td>0.090801</td>\n",
       "      <td>0.085902</td>\n",
       "      <td>0.043810</td>\n",
       "      <td>0.068849</td>\n",
       "      <td>0.085182</td>\n",
       "      <td>no_preprocessing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.111803</td>\n",
       "      <td>583</td>\n",
       "      <td>0.729016</td>\n",
       "      <td>0.028632</td>\n",
       "      <td>0.241439</td>\n",
       "      <td>0.176463</td>\n",
       "      <td>0.113341</td>\n",
       "      <td>0.091900</td>\n",
       "      <td>0.937996</td>\n",
       "      <td>0.749737</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>0.626857</td>\n",
       "      <td>momentum</td>\n",
       "      <td>0.010333</td>\n",
       "      <td>0.018887</td>\n",
       "      <td>0.063483</td>\n",
       "      <td>0.074249</td>\n",
       "      <td>0.076099</td>\n",
       "      <td>0.063471</td>\n",
       "      <td>no_preprocessing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.113413</td>\n",
       "      <td>325</td>\n",
       "      <td>0.505720</td>\n",
       "      <td>0.643632</td>\n",
       "      <td>0.205732</td>\n",
       "      <td>0.483992</td>\n",
       "      <td>0.520969</td>\n",
       "      <td>0.400504</td>\n",
       "      <td>0.723638</td>\n",
       "      <td>0.092775</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>0.802996</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.051419</td>\n",
       "      <td>0.048282</td>\n",
       "      <td>0.050930</td>\n",
       "      <td>0.040379</td>\n",
       "      <td>0.064292</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>no_preprocessing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    performance  batch_size     beta1     beta2  dropout_layer_1  \\\n",
       "26     0.078308         854  0.196384  0.913762         0.084144   \n",
       "44     0.082602         275  0.676804  0.574869         0.577597   \n",
       "7      0.082664         185  0.274779  0.058622         0.858087   \n",
       "54     0.083246         724  0.314150  0.634334         0.335285   \n",
       "11     0.085101         175  0.907804  0.758103         0.808820   \n",
       "43     0.087579         445  0.741600  0.522028         0.525227   \n",
       "23     0.090063         228  0.432863  0.431075         0.457242   \n",
       "60     0.091320         271  0.786533  0.277555         0.208178   \n",
       "42     0.092548         270  0.953142  0.160750         0.588261   \n",
       "53     0.093169         171  0.152366  0.650134         0.808500   \n",
       "16     0.093184         371  0.961718  0.073293         0.510738   \n",
       "25     0.098759         397  0.368593  0.805288         0.599187   \n",
       "15     0.099303         206  0.036937  0.406246         0.843102   \n",
       "52     0.100592         171  0.152366  0.650134         0.808500   \n",
       "39     0.101266         826  0.559234  0.795851         0.215344   \n",
       "59     0.102501         394  0.764309  0.500972         0.468970   \n",
       "38     0.104427         152  0.426774  0.378753         0.357967   \n",
       "32     0.107462         164  0.558045  0.405515         0.611085   \n",
       "22     0.111803         583  0.729016  0.028632         0.241439   \n",
       "41     0.113413         325  0.505720  0.643632         0.205732   \n",
       "\n",
       "    dropout_layer_2  dropout_layer_3  dropout_layer_4  dropout_layer_5  \\\n",
       "26         0.861973         0.171859         0.904045         0.019498   \n",
       "44         0.589949         0.888228         0.712073         0.262108   \n",
       "7          0.667039         0.877792         0.357267         0.682999   \n",
       "54         0.566046         0.204832         0.290946         0.848140   \n",
       "11         0.965795         0.019026         0.403560         0.033201   \n",
       "43         0.609476         0.956937         0.893637         0.324273   \n",
       "23         0.064014         0.126367         0.287198         0.543494   \n",
       "60         0.828025         0.512164         0.809649         0.492633   \n",
       "42         0.135091         0.321836         0.281384         0.294965   \n",
       "53         0.881198         0.285800         0.593314         0.074184   \n",
       "16         0.473444         0.545795         0.510467         0.041528   \n",
       "25         0.775147         0.631153         0.247123         0.861399   \n",
       "15         0.010373         0.035298         0.657890         0.057009   \n",
       "52         0.881198         0.285800         0.593314         0.074184   \n",
       "39         0.818202         0.793165         0.885243         0.396036   \n",
       "59         0.708864         0.571474         0.600712         0.752166   \n",
       "38         0.632846         0.112174         0.142758         0.230819   \n",
       "32         0.723711         0.916237         0.545445         0.268705   \n",
       "22         0.176463         0.113341         0.091900         0.937996   \n",
       "41         0.483992         0.520969         0.400504         0.723638   \n",
       "\n",
       "    dropout_layer_6        ...         number_epochs       rho    solver  \\\n",
       "26         0.921645        ...                    10  0.017938      adam   \n",
       "44         0.909249        ...                    10  0.413255  momentum   \n",
       "7          0.958286        ...                     6  0.100290  momentum   \n",
       "54         0.514260        ...                    10  0.118770   adagrad   \n",
       "11         0.548311        ...                    10  0.869560       sgd   \n",
       "43         0.080875        ...                     3  0.820418  momentum   \n",
       "23         0.825455        ...                    10  0.375674      adam   \n",
       "60         0.000129        ...                    20  0.526185   adagrad   \n",
       "42         0.164420        ...                    20  0.552695  nesterov   \n",
       "53         0.274967        ...                    18  0.464284  nesterov   \n",
       "16         0.223776        ...                    18  0.269312   adagrad   \n",
       "25         0.531999        ...                    20  0.027757      adam   \n",
       "15         0.223576        ...                    16  0.354519  nesterov   \n",
       "52         0.274967        ...                    18  0.464284  nesterov   \n",
       "39         0.275218        ...                     5  0.248684   adagrad   \n",
       "59         0.401412        ...                    13  0.361452  adadelta   \n",
       "38         0.156980        ...                     4  0.047387   adagrad   \n",
       "32         0.530799        ...                    17  0.461751      adam   \n",
       "22         0.749737        ...                    13  0.626857  momentum   \n",
       "41         0.092775        ...                    14  0.802996   adagrad   \n",
       "\n",
       "    std_layer_1  std_layer_2  std_layer_3  std_layer_4  std_layer_5  \\\n",
       "26     0.032325     0.021721     0.067461     0.086482     0.087822   \n",
       "44     0.058553     0.040004     0.053026     0.053220     0.088900   \n",
       "7      0.044592     0.062359     0.010090     0.099593     0.007928   \n",
       "54     0.016347     0.031210     0.058050     0.089883     0.014399   \n",
       "11     0.087493     0.095189     0.097195     0.065028     0.072724   \n",
       "43     0.007341     0.022111     0.029298     0.057126     0.026064   \n",
       "23     0.060206     0.082668     0.010651     0.097712     0.086164   \n",
       "60     0.041382     0.029106     0.058355     0.076481     0.021931   \n",
       "42     0.023848     0.071060     0.096307     0.040230     0.086203   \n",
       "53     0.030270     0.091690     0.025418     0.070358     0.037071   \n",
       "16     0.053687     0.060496     0.074062     0.066738     0.068031   \n",
       "25     0.078539     0.026020     0.053881     0.066439     0.006898   \n",
       "15     0.095345     0.080537     0.072212     0.075598     0.002793   \n",
       "52     0.030270     0.091690     0.025418     0.070358     0.037071   \n",
       "39     0.059479     0.040990     0.056198     0.082273     0.027011   \n",
       "59     0.055455     0.093656     0.031279     0.053957     0.079899   \n",
       "38     0.057602     0.026257     0.061049     0.035945     0.080969   \n",
       "32     0.087097     0.090801     0.085902     0.043810     0.068849   \n",
       "22     0.010333     0.018887     0.063483     0.074249     0.076099   \n",
       "41     0.051419     0.048282     0.050930     0.040379     0.064292   \n",
       "\n",
       "    std_layer_6      preprocessor  \n",
       "26     0.071028  no_preprocessing  \n",
       "44     0.006481  no_preprocessing  \n",
       "7      0.025462  no_preprocessing  \n",
       "54     0.011898  no_preprocessing  \n",
       "11     0.045124  no_preprocessing  \n",
       "43     0.072571  no_preprocessing  \n",
       "23     0.054308  no_preprocessing  \n",
       "60     0.062448  no_preprocessing  \n",
       "42     0.097796  no_preprocessing  \n",
       "53     0.007976  no_preprocessing  \n",
       "16     0.055623  no_preprocessing  \n",
       "25     0.079550  no_preprocessing  \n",
       "15     0.090751  no_preprocessing  \n",
       "52     0.007976  no_preprocessing  \n",
       "39     0.037775  no_preprocessing  \n",
       "59     0.079330  no_preprocessing  \n",
       "38     0.050337  no_preprocessing  \n",
       "32     0.085182  no_preprocessing  \n",
       "22     0.063471  no_preprocessing  \n",
       "41     0.001541  no_preprocessing  \n",
       "\n",
       "[20 rows x 30 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coded_df.sort_values(by='performance').head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Don't know if should purge num_units, dropout and std from configs\n",
    "# Recode the categorical features to integers\n",
    "coded_df[['solver','preprocessor']] = coded_df[['solver','preprocessor']].apply(lambda x: x.cat.codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_matrix = coded_df.ix[:,1:-1].values\n",
    "performance_array = coded_df.ix[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(performance_array[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>performance</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>beta1</th>\n",
       "      <th>beta2</th>\n",
       "      <th>dropout_layer_1</th>\n",
       "      <th>dropout_layer_2</th>\n",
       "      <th>dropout_layer_3</th>\n",
       "      <th>dropout_layer_4</th>\n",
       "      <th>dropout_layer_5</th>\n",
       "      <th>dropout_layer_6</th>\n",
       "      <th>...</th>\n",
       "      <th>number_epochs</th>\n",
       "      <th>rho</th>\n",
       "      <th>solver</th>\n",
       "      <th>std_layer_1</th>\n",
       "      <th>std_layer_2</th>\n",
       "      <th>std_layer_3</th>\n",
       "      <th>std_layer_4</th>\n",
       "      <th>std_layer_5</th>\n",
       "      <th>std_layer_6</th>\n",
       "      <th>preprocessor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>100</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>extra_trees_preproc_for_classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.921571</td>\n",
       "      <td>259</td>\n",
       "      <td>0.458778</td>\n",
       "      <td>0.121429</td>\n",
       "      <td>0.519209</td>\n",
       "      <td>0.411903</td>\n",
       "      <td>0.660771</td>\n",
       "      <td>0.824662</td>\n",
       "      <td>0.608685</td>\n",
       "      <td>0.989170</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.740881</td>\n",
       "      <td>momentum</td>\n",
       "      <td>0.066318</td>\n",
       "      <td>0.026614</td>\n",
       "      <td>0.091562</td>\n",
       "      <td>0.041221</td>\n",
       "      <td>0.036893</td>\n",
       "      <td>0.080055</td>\n",
       "      <td>extra_trees_preproc_for_classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.137284</td>\n",
       "      <td>230</td>\n",
       "      <td>0.795609</td>\n",
       "      <td>0.064604</td>\n",
       "      <td>0.356252</td>\n",
       "      <td>0.512411</td>\n",
       "      <td>0.922523</td>\n",
       "      <td>0.212629</td>\n",
       "      <td>0.415022</td>\n",
       "      <td>0.192982</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.635820</td>\n",
       "      <td>momentum</td>\n",
       "      <td>0.034126</td>\n",
       "      <td>0.030248</td>\n",
       "      <td>0.008640</td>\n",
       "      <td>0.082886</td>\n",
       "      <td>0.015721</td>\n",
       "      <td>0.076149</td>\n",
       "      <td>extra_trees_preproc_for_classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.103114</td>\n",
       "      <td>348</td>\n",
       "      <td>0.984609</td>\n",
       "      <td>0.574990</td>\n",
       "      <td>0.028903</td>\n",
       "      <td>0.640310</td>\n",
       "      <td>0.854954</td>\n",
       "      <td>0.322164</td>\n",
       "      <td>0.970398</td>\n",
       "      <td>0.255932</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.629816</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.036537</td>\n",
       "      <td>0.030028</td>\n",
       "      <td>0.072650</td>\n",
       "      <td>0.055942</td>\n",
       "      <td>0.034039</td>\n",
       "      <td>0.022942</td>\n",
       "      <td>extra_trees_preproc_for_classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.102516</td>\n",
       "      <td>279</td>\n",
       "      <td>0.561777</td>\n",
       "      <td>0.536513</td>\n",
       "      <td>0.521723</td>\n",
       "      <td>0.013532</td>\n",
       "      <td>0.299414</td>\n",
       "      <td>0.301125</td>\n",
       "      <td>0.306639</td>\n",
       "      <td>0.141634</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.779810</td>\n",
       "      <td>nesterov</td>\n",
       "      <td>0.087197</td>\n",
       "      <td>0.009854</td>\n",
       "      <td>0.051698</td>\n",
       "      <td>0.015702</td>\n",
       "      <td>0.083680</td>\n",
       "      <td>0.044015</td>\n",
       "      <td>extra_trees_preproc_for_classification</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   performance  batch_size     beta1     beta2  dropout_layer_1  \\\n",
       "0     1.000000         100  0.900000  0.900000         0.500000   \n",
       "1     0.921571         259  0.458778  0.121429         0.519209   \n",
       "2     0.137284         230  0.795609  0.064604         0.356252   \n",
       "3     0.103114         348  0.984609  0.574990         0.028903   \n",
       "4     0.102516         279  0.561777  0.536513         0.521723   \n",
       "\n",
       "   dropout_layer_2  dropout_layer_3  dropout_layer_4  dropout_layer_5  \\\n",
       "0         0.500000         0.500000         0.500000         0.500000   \n",
       "1         0.411903         0.660771         0.824662         0.608685   \n",
       "2         0.512411         0.922523         0.212629         0.415022   \n",
       "3         0.640310         0.854954         0.322164         0.970398   \n",
       "4         0.013532         0.299414         0.301125         0.306639   \n",
       "\n",
       "   dropout_layer_6                   ...                    number_epochs  \\\n",
       "0         0.500000                   ...                                3   \n",
       "1         0.989170                   ...                                7   \n",
       "2         0.192982                   ...                                9   \n",
       "3         0.255932                   ...                                9   \n",
       "4         0.141634                   ...                                6   \n",
       "\n",
       "        rho    solver  std_layer_1  std_layer_2  std_layer_3  std_layer_4  \\\n",
       "0  0.950000       sgd     0.005000     0.005000     0.005000     0.005000   \n",
       "1  0.740881  momentum     0.066318     0.026614     0.091562     0.041221   \n",
       "2  0.635820  momentum     0.034126     0.030248     0.008640     0.082886   \n",
       "3  0.629816   adagrad     0.036537     0.030028     0.072650     0.055942   \n",
       "4  0.779810  nesterov     0.087197     0.009854     0.051698     0.015702   \n",
       "\n",
       "   std_layer_5  std_layer_6                            preprocessor  \n",
       "0     0.005000     0.005000  extra_trees_preproc_for_classification  \n",
       "1     0.036893     0.080055  extra_trees_preproc_for_classification  \n",
       "2     0.015721     0.076149  extra_trees_preproc_for_classification  \n",
       "3     0.034039     0.022942  extra_trees_preproc_for_classification  \n",
       "4     0.083680     0.044015  extra_trees_preproc_for_classification  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Directory to save the files used in RfRun\n",
    "dir_to_store = '/home/mendozah/workspace/parameters_curves'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(os.path.join(dir_to_store, 'data_matrix'), data_matrix)\n",
    "np.save(os.path.join(dir_to_store, 'performance'), performance_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the files as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = np.load(os.path.join(dir_to_store, 'data_matrix.npy'))\n",
    "e = np.load(os.path.join(dir_to_store, 'performance.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(os.path.join(dir_to_store, 'features_matrix.csv'), d, delimiter=',')\n",
    "np.savetxt(os.path.join(dir_to_store, 'error_response.csv'), e, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>363.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.177049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.575083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.205173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.219414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.102783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.100091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.603480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.930794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.487874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.070665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.634390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1640.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1939.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2978.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>486.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5589.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5030.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.201536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.035165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.070949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.032753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.079600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.046433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.023245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0\n",
       "0    363.000000\n",
       "1      0.177049\n",
       "2      0.575083\n",
       "3      0.205173\n",
       "4      0.219414\n",
       "5      0.102783\n",
       "6      0.100091\n",
       "7      0.603480\n",
       "8      0.930794\n",
       "9      0.487874\n",
       "10     0.070665\n",
       "11     0.634390\n",
       "12     4.000000\n",
       "13  1640.000000\n",
       "14  1939.000000\n",
       "15  2978.000000\n",
       "16   486.000000\n",
       "17  5589.000000\n",
       "18  5030.000000\n",
       "19    20.000000\n",
       "20     0.201536\n",
       "21     4.000000\n",
       "22     0.035165\n",
       "23     0.070949\n",
       "24     0.032753\n",
       "25     0.079600\n",
       "26     0.046433\n",
       "27     0.023245"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(d[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
